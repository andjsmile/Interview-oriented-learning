#### 1.SGD，Momentum,Adagard，Adam 等
**自己的一篇博客** 
[点我，点我](https://www.cnblogs.com/GeekDanny/p/9655597.html)

SGD:随机梯度下降 ，每次计算数据集的mini-batch的梯度，来更新参数  mini-BGD
Momentum:参考物理动量的概念，前几次的梯度也会参与到当前的计算，前几轮的梯度叠加在计算中会有一定的衰减
Adagard:在学习中可以自动的变更学习的rate，,设置一个全局的学习率,而实际的学习率与以往的参数模和的开方成反比
Adam:利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率.在经过偏置的校正后,每一次迭代后的学习率都有个确定的范围,使得参数较为平稳。


#### 2.L1不可导的时候该怎么办

当损失函数不可导，梯度下降不在有效，可以使用**坐标下降法**
- 定义:坐标下降法是一种非梯度优化，在每步迭代中沿一个坐标方向进行搜索，循环使用不同的坐标方向来达到目标函数局部最小值
(求解极大值的问题也称为 坐标上升法)

- 使用方法:坐标轴下降法是沿着坐标轴的方向,假设有m个特征个数,坐标轴下降法进参数更新的时候,先固定m-1个值,然后再求另外一个的局部最优解,从而避免损失函数不可导问题


#### 3.sigmoid函数

- sigmoid函数是一类函数，形似S的函数
- sigmoid函数
    - logistic 函数，对数几率函数是最重要的代表
    - 双曲正切函数
    - 等等

- 性质
   - 定义域 (-∞,+∞)
   - 值域  （-1，1）
   - 定义域内连续，光滑可导函数。
   - f(x)'=f(x)(1-f(x)), 很重要的性质




