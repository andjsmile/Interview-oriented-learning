#### 计算两个矩阵的欧式距离

[自己博客](https://www.cnblogs.com/GeekDanny/p/10179251.html#4228516)
- 继续学习 类似于平方差公式

矩阵A mxk 矩阵B nxk
- 1.先求矩阵AB^T   mxn
- 2.矩阵A，矩阵B^T求出每个向量的模平方
- 3.矩阵A和B^T进行拓展称为  mxn
- 4.A+B^T-2*AB^T

代码实现 L2distanceWithMatrix.py

#### 欧拉公式

e^(jx)=cosx+jsinx

#### 矩阵正定性的判定

矩阵的所有特征值均不小于0，矩阵半正定
矩阵的所有特征值都大于0，矩阵正定

**Hessian 矩阵正定性在梯度下降中的应用**

- hessian矩阵:多变量实值函数的二阶偏导数组成的方块矩阵

在判断优化算法的可行性时Hessian矩阵的正定性起到了很大的作用,若Hessian正定,则函数的二阶偏导恒大于0,函数的变化率处于递增状态,在牛顿法等梯度下降的方法中,Hessian矩阵的正定性可以很容易的判断函数是否可收敛到局部或全局最优解


#### PCA principal component analysis

主成分分析是一种常见的**降维分析**
- 目的:将数据从原来的坐标系转换到新的坐标系，新坐标的选择和数据本身相关。第一个新坐标轴是原数据中方差最大的方向，第二个坐标选择和第一个坐标正交且具有最大方差的方向。
- 操作:PCA算法的具体操作为对所有的样本进行中心化操作,计算样本的协方差矩阵,然后对协方差矩阵做特征值分解,取最大的n个特征值对应的特征向量构造投影矩阵

#### 拟牛顿法

**牛顿法**
- 牛顿法采用了二阶导数，在每一轮的迭代中涉及到海森矩阵的求逆，时间复杂度较高
- 牛顿法的收敛速度快,迭代次数少,但是Hessian矩阵很稠密时,每次迭代的计算量很大,随着数据规模增大,Hessian矩阵也会变大,需要更多的存储空间以及计算量

**拟牛顿法**
- 拟牛顿法就是在牛顿法的基础上引入了Hessian矩阵的近似矩阵,避免了每次都计算Hessian矩阵的逆
- Hessian矩阵的逆矩阵来代替Hessian矩阵,虽然不能像牛顿法那样保证最优化的方向,但其逆矩阵始终是正定的,因此算法始终朝最优化的方向搜索

#### 编辑距离 

**概念**

- 编辑距离，又称Levenshtein距离（莱文斯坦距离也叫做Edit Distance），是指两个字串之间，由一个转成另一个所需的最少编辑操作次数，如果它们的距离越大，说明它们越是不同。许可的编辑操作包括将一个字符替换成另一个字符，插入一个字符，删除一个字符

**公式**
- 如果str1=”ivan1”，str2=”ivan2”，那么经过计算后等于1。str1的”1”转换”2”，转换了一个字符，所以距离是1，相似度=1-1/Math.Max(str1.length,str2.length)=0.8
**用途**

- 只有替换、插入、删除三个操作
- 编辑距离用来比较两个字符串的相似度的

动态规划思想，参考程序和IT面试指南


